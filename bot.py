# √âtape 1 : Importer les biblioth√®ques dont on a besoin
import os
import requests
from bs4 import BeautifulSoup
import time
import random
import re
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse
from supabase import create_client, Client 

# √âtape 2 : Configurer nos variables
DEALABS_URL = "https://www.dealabs.com/hot"

# Configuration des secrets via variables d'environnement
DISCORD_WEBHOOK_URL = os.getenv("DISCORD_WEBHOOK_URL")
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

# PARAM√àTRES DE FILTRAGE
TEMPERATURE_MINIMUM = 100  # Temp√©rature minimum pour traiter un deal
TEMPERATURE_PREMIUM = 500  # Temp√©rature pour les deals "premium"
TEMPERATURE_HOT = 1000     # Temp√©rature pour les deals "tr√®s hot"

# PARAM√àTRES D'AFFILIATION
AMAZON_TAG = "dealbot00-21"  # Remplacez par votre vrai tag Amazon

# Headers pour √©viter la d√©tection anti-bot
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'fr-FR,fr;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate',
    'DNT': '1',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
    'Referer': 'https://www.google.com/'
}

def scraper_dealabs():
    """
    Trouve et retourne le premier deal valide de Dealabs
    """
    print("üîç Tentative de scraping de Dealabs...")
    
    try:
        time.sleep(random.uniform(1, 3))  # D√©lai al√©atoire
        
        session = requests.Session()
        session.headers.update(HEADERS)
        
        response = session.get(DEALABS_URL, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # S√©lecteurs CSS pour trouver les deals - Version am√©lior√©e
        selectors_to_try = [
            'article[data-thread-id]',  # S√©lecteur le plus fiable
            'div[data-thread-id]',
            'article[class*="thread"]',
            'div[class*="thread"]',
            'article.thread--card',
            'article',
            '.threadGrid article',
            '.threadList article'
        ]
        
        all_deals = []
        for selector in selectors_to_try:
            all_deals = soup.select(selector)
            if all_deals:
                print(f"‚úÖ Trouv√© {len(all_deals)} √©l√©ments avec le s√©lecteur: {selector}")
                break
        
        if not all_deals:
            print("‚ùå Aucun √©l√©ment trouv√©")
            return None, None, 0
        
        # Analyser chaque deal
        for i, deal_element in enumerate(all_deals[:20]):
            print(f"üîç Analyse de l'√©l√©ment {i+1}...")
            
            # √âTAPE 1 : Extraire la temp√©rature
            temperature = extraire_temperature(deal_element)
            print(f"üå°Ô∏è Temp√©rature d√©tect√©e : {temperature}¬∞")
            
            # Filtrer par temp√©rature
            if temperature < TEMPERATURE_MINIMUM:
                print(f"‚ùÑÔ∏è Deal ignor√© (temp√©rature {temperature}¬∞ < {TEMPERATURE_MINIMUM}¬∞)")
                continue
            
            # √âTAPE 2 : Extraire le lien du deal - Version am√©lior√©e
            link_selectors = [
                'a[class*="thread-link"]',
                'a[href*="/deals/"]', 
                'a[href*="/bons-plans/"]',
                'a[href*="/codes-promo/"]',
                'a[href*="/gratuit/"]',
                'h1 a', 'h2 a', 'h3 a',  # Titre cliquable
                'a[title]',
                'a'
            ]
            
            link_tag = None
            for link_selector in link_selectors:
                link_tag = deal_element.select_one(link_selector)
                if link_tag and 'href' in link_tag.attrs:
                    href = link_tag.get('href', '')
                    # V√©rifier que c'est bien un lien vers un deal
                    if any(keyword in href.lower() for keyword in ['/deals/', '/bons-plans/', '/codes-promo/', '/gratuit/']):
                        break
            
            if not link_tag:
                print("‚ùå Aucun lien trouv√©")
                continue
            
            # √âTAPE 3 : Extraire le titre - Version am√©lior√©e
            title_selectors = [
                'strong[class*="thread-title"]',
                'h1', 'h2', 'h3',
                'strong', 
                '[class*="title"]', 
                'span[class*="title"]',
                'a span'
            ]
            
            title_text = None
            for title_selector in title_selectors:
                title_element = deal_element.select_one(title_selector)
                if title_element:
                    title_text = title_element.get_text(strip=True)
                    if title_text and len(title_text) > 10:
                        break
            
            # Fallback pour le titre
            if not title_text or len(title_text) < 10:
                title_text = link_tag.get('title', '').strip()
                if not title_text:
                    title_text = link_tag.get_text(strip=True)
            
            # √âTAPE 4 : Construire l'URL compl√®te
            if title_text and len(title_text) > 5:
                deal_link = link_tag['href']
                
                if not deal_link.startswith('http'):
                    deal_link = "https://www.dealabs.com" + deal_link
                
                temp_emoji = get_temperature_emoji(temperature)
                print(f"‚úÖ Deal {temp_emoji} trouv√© ({temperature}¬∞) : {title_text[:80]}{'...' if len(title_text) > 80 else ''}")
                return title_text, deal_link, temperature
        
        print("‚ùå Aucun deal valide trouv√©")
        return None, None, 0
        
    except Exception as e:
        print(f"‚ùå Erreur lors du scraping : {e}")
        return None, None, 0

def extraire_temperature(deal_element):
    """
    Extrait la temp√©rature d'un √©l√©ment de deal - Version am√©lior√©e
    """
    # S√©lecteurs sp√©cifiques pour la temp√©rature sur Dealabs
    temp_selectors = [
        'span[class*="vote-temp"]',
        'span[class*="temperature"]', 
        'div[class*="temperature"]',
        'span[class*="vote-box"]',
        '[class*="vote-"] span',
        '[data-vote]',
        '[class*="temp"]', 
        '[class*="vote"]', 
        '[class*="score"]', 
        'span[title*="¬∞"]',
        '.vote-box span',
        '.vote-temp',
        '*'
    ]
    
    for selector in temp_selectors:
        try:
            elements = deal_element.select(selector)
            for element in elements:
                # Chercher dans le texte
                text = element.get_text(strip=True)
                temperature = extraire_nombre_temperature(text)
                if temperature > 0:
                    return temperature
                
                # Chercher dans les attributs
                for attr_name, attr_value in element.attrs.items():
                    if isinstance(attr_value, str):
                        temperature = extraire_nombre_temperature(attr_value)
                        if temperature > 0:
                            return temperature
                    elif isinstance(attr_value, list):
                        for val in attr_value:
                            temperature = extraire_nombre_temperature(str(val))
                            if temperature > 0:
                                return temperature
        except:
            continue
    
    return 0

def extraire_nombre_temperature(text):
    """
    Extrait un nombre de temp√©rature d'un texte - Version am√©lior√©e
    """
    if not text:
        return 0
        
    patterns = [
        r'(\d+)¬∞',           # 123¬∞
        r'(\d+)\s*¬∞',        # 123 ¬∞
        r'\+(\d+)',          # +123
        r'temp[^0-9]*(\d+)', # temp: 123
        r'vote[^0-9]*(\d+)', # vote: 123
        r'score[^0-9]*(\d+)',# score: 123
        r'(\d+)\s*points?',  # 123 points
        r'(\d+)\s*votes?',   # 123 votes
        r'(\d{2,4})',        # 2-4 chiffres cons√©cutifs
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, text.lower())
        if matches:
            try:
                numbers = [int(match) for match in matches]
                # Filtrer les nombres raisonnables pour une temp√©rature
                valid_temps = [n for n in numbers if 0 <= n <= 9999]
                if valid_temps:
                    return max(valid_temps)
            except ValueError:
                continue
    
    return 0

def get_temperature_emoji(temperature):
    """
    Retourne un √©moji selon la temp√©rature
    """
    if temperature >= TEMPERATURE_HOT:
        return "üî•üî•üî•"
    elif temperature >= TEMPERATURE_PREMIUM:
        return "üî•üî•"
    elif temperature >= TEMPERATURE_MINIMUM:
        return "üî•"
    else:
        return "‚ùÑÔ∏è"

def deal_existe_en_bdd(supabase, deal_link):
    """
    V√©rifie si un deal existe d√©j√† en base de donn√©es
    """
    try:
        print(f"üîç V√©rification en BDD pour : {deal_link[:60]}...")
        
        response = supabase.table('deals_envoyes').select('id, titre').eq('deal_link', deal_link).execute()
        existe = len(response.data) > 0
        
        if existe:
            deal_info = response.data[0]
            print(f"ü•± Deal d√©j√† trait√© : '{deal_info.get('titre', 'Sans titre')[:50]}...'")
            return True
        else:
            print("‚ú® Nouveau deal d√©tect√©, pas encore en BDD !")
            return False
            
    except Exception as e:
        print(f"‚ùå Erreur BDD lors de la v√©rification : {e}")
        print("‚ö†Ô∏è  Par pr√©caution, on consid√®re le deal comme existant pour √©viter les doublons")
        return True

def recuperer_lien_marchand(url_dealabs):
    """
    Visite la page du deal et r√©cup√®re le lien vers le marchand - Version corrig√©e
    """
    print(f"üïµÔ∏è‚Äç‚ôÇÔ∏è Recherche du lien marchand sur {url_dealabs[:50]}...")
    
    try:
        time.sleep(random.uniform(1, 2))
        
        session = requests.Session()
        session.headers.update(HEADERS)
        response = session.get(url_dealabs, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # M√âTHODE 1: Chercher les boutons "Voir le deal" avec de meilleurs s√©lecteurs
        bouton_selectors = [
            'a[class*="cept-dealBtn"]',           # S√©lecteur principal Dealabs
            'a[class*="dealBtn"]',
            'a[class*="thread-deal"]',
            'a[data-handler="goToLink"]',         # Handler JavaScript
            'a[href*="out.php"]',                 # Lien de redirection Dealabs
            'a[href*="/visit/"]',                 # Autre type de redirection
            'a[class*="goToLink"]',
            'a.btn[href*="dealabs.com"]',
            '.deal-btn a',
            '.thread-deal-btn a',
            '.cept-dealBtn',
            'a[title*="aller"]',
            'a[title*="voir"]',
        ]
        
        lien_marchand = None
        
        for selector in bouton_selectors:
            boutons = soup.select(selector)
            for bouton in boutons:
                href = bouton.get('href', '')
                if href and ('out.php' in href or '/visit/' in href or any(domain in href for domain in ['amazon.fr', 'fnac.com', 'cdiscount.com'])):
                    if not href.startswith('http'):
                        href = "https://www.dealabs.com" + href
                    
                    print(f"‚úÖ Lien de redirection trouv√© : {href[:70]}...")
                    
                    # M√âTHODE 2: Suivre la redirection pour obtenir l'URL finale
                    lien_final = suivre_redirection(href, session)
                    if lien_final:
                        return lien_final
                    else:
                        lien_marchand = href  # Fallback
                        
        # M√âTHODE 3: Chercher directement les liens vers des marchands connus
        if not lien_marchand:
            print("üîç Recherche directe des liens marchands...")
            liens_directs = soup.find_all('a', href=True)
            
            marchands = ['amazon.fr', 'fnac.com', 'cdiscount.com', 'darty.com', 'boulanger.com', 'ldlc.com']
            
            for link in liens_directs:
                href = link.get('href', '')
                if any(marchand in href.lower() for marchand in marchands):
                    if not href.startswith('http'):
                        continue
                    print(f"‚úÖ Lien marchand direct trouv√© : {href[:70]}...")
                    return href
        
        if lien_marchand:
            print(f"‚úÖ Lien de redirection retourn√© : {lien_marchand[:70]}...")
            return lien_marchand
        else:
            print("‚ùå Aucun lien marchand trouv√©")
            return None
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la r√©cup√©ration du lien marchand : {e}")
        return None

def suivre_redirection(url_redirection, session):
    """
    Suit une redirection Dealabs pour obtenir l'URL finale du marchand
    """
    try:
        print(f"üîÑ Suivi de la redirection : {url_redirection[:50]}...")
        
        # Suivre les redirections sans t√©l√©charger le contenu complet
        response = session.head(url_redirection, allow_redirects=True, timeout=10)
        
        if response.url != url_redirection:
            print(f"‚úÖ URL finale apr√®s redirection : {response.url[:70]}...")
            return response.url
        else:
            # Si pas de redirection automatique, essayer avec GET
            response = session.get(url_redirection, timeout=10, allow_redirects=True)
            if response.url != url_redirection:
                print(f"‚úÖ URL finale apr√®s redirection GET : {response.url[:70]}...")
                return response.url
                
    except Exception as e:
        print(f"‚ö†Ô∏è  Erreur lors du suivi de redirection : {e}")
    
    return None

def transformer_en_lien_affilie(url_marchand):
    """
    Transforme une URL en lien affili√© si possible - Version am√©lior√©e
    """
    if not url_marchand:
        return None
    
    print(f"üí∞ Analyse du lien pour affiliation : {url_marchand[:50]}...")
    
    try:
        # Amazon France
        if "amazon.fr" in url_marchand.lower():
            parsed_url = urlparse(url_marchand)
            query_params = parse_qs(parsed_url.query)
            
            # Nettoyer et ajouter notre tag
            query_params['tag'] = [AMAZON_TAG]
            
            # Supprimer les param√®tres de tracking externes
            params_to_remove = ['ref', 'ref_', 'pf_rd_p', 'pf_rd_r', 'pd_rd_i', 'pd_rd_r', 'pd_rd_w', 'pd_rd_wg']
            for param in params_to_remove:
                query_params.pop(param, None)
            
            new_query = urlencode(query_params, doseq=True)
            lien_affilie = urlunparse((parsed_url.scheme, parsed_url.netloc, parsed_url.path, parsed_url.params, new_query, parsed_url.fragment))
            
            print(f"‚úÖ Lien Amazon affili√© cr√©√© avec tag : {AMAZON_TAG}")
            return lien_affilie
        
        # Fnac (Awin/Commission Junction)
        elif "fnac.com" in url_marchand.lower():
            # Exemple d'ajout d'affiliation Fnac (remplacez par vos vrais param√®tres)
            if "?" in url_marchand:
                lien_affilie = f"{url_marchand}&awc=VOTRE_ID_AWIN"
            else:
                lien_affilie = f"{url_marchand}?awc=VOTRE_ID_AWIN"
            print("‚ÑπÔ∏è  Fnac - ajoutez votre vrai ID Awin")
            return lien_affilie
        
        # Cdiscount
        elif "cdiscount.com" in url_marchand.lower():
            # Exemple d'ajout d'affiliation Cdiscount
            print("‚ÑπÔ∏è  Cdiscount d√©tect√© - ajoutez votre logique d'affiliation")
            return url_marchand
        
        # Autres marchands
        else:
            print("‚ÑπÔ∏è  Marchand non partenaire, lien original conserv√©")
            return url_marchand
            
    except Exception as e:
        print(f"‚ö†Ô∏è  Erreur lors de la transformation en lien affili√© : {e}")
        return url_marchand

def envoyer_notification_discord(titre, lien_final, temperature=0, lien_dealabs=None):
    """
    Envoie une notification Discord avec le lien final (affili√© si possible)
    """
    # Limiter la longueur du titre
    if len(titre) > 180:
        titre = titre[:177] + "..."
    
    temp_emoji = get_temperature_emoji(temperature)
    temp_text = f" ({temperature}¬∞)" if temperature > 0 else ""
    
    # Message selon la temp√©rature
    if temperature >= TEMPERATURE_HOT:
        intro = f"üö® **DEAL TR√àS HOT D√âTECT√â !** {temp_emoji}"
    elif temperature >= TEMPERATURE_PREMIUM:
        intro = f"üî• **EXCELLENT DEAL D√âTECT√â !** {temp_emoji}"
    else:
        intro = f"‚ú® **Nouveau Deal D√©tect√© !** {temp_emoji}"
    
    # Indiquer le type de lien
    lien_info = ""
    if lien_final:
        if "amazon.fr" in lien_final and AMAZON_TAG in lien_final:
            lien_info = "üí∞ **Lien Amazon affili√© :** "
        elif any(marchand in lien_final.lower() for marchand in ['fnac.com', 'cdiscount.com']):
            lien_info = "üõí **Lien marchand :** "
        elif "dealabs.com" in lien_final:
            lien_info = "üîó **Lien Dealabs :** "
        else:
            lien_info = "üîó **Lien :** "
    
    # Construction du message
    message = f"{intro}\n\n**{titre}**{temp_text}\n\n{lien_info}{lien_final}"
    
    # Ajouter le lien Dealabs si diff√©rent
    if lien_dealabs and lien_final != lien_dealabs and not "dealabs.com" in lien_final:
        message += f"\nüìã **Voir sur Dealabs :** {lien_dealabs}"
    
    data = {
        "content": message,
        "username": "DealBot Pro",
        "avatar_url": "https://cdn-icons-png.flaticon.com/512/3159/3159310.png"
    }

    print("üì§ Envoi de la notification √† Discord...")
    try:
        response = requests.post(DISCORD_WEBHOOK_URL, json=data, timeout=10)
        response.raise_for_status()
        print("‚úÖ Notification envoy√©e avec succ√®s !")
        return True
    except Exception as e:
        print(f"‚ùå Erreur lors de l'envoi √† Discord : {e}")
        return False

def sauvegarder_deal_traite(supabase, deal_link, titre="", temperature=0, lien_marchand=""):
    """
    Sauvegarde le deal trait√© en base de donn√©es - Version corrig√©e
    """
    try:
        # Structure de base des donn√©es (sans created_at car g√©r√© automatiquement par Supabase)
        data = {
            "deal_link": deal_link,
            "titre": titre[:200],  # Limiter la longueur
            "temperature": temperature
        }
        
        # Ajouter le lien marchand seulement s'il existe et si la colonne existe
        if lien_marchand:
            data["lien_marchand"] = lien_marchand[:500]
        
        # Utiliser upsert pour √©viter les erreurs de doublons
        result = supabase.table('deals_envoyes').upsert(data, on_conflict="deal_link").execute()
        print("üíæ Deal sauvegard√© en base de donn√©es")
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la sauvegarde : {e}")
        print(f"üîß Tentative avec structure minimale...")
        
        # Fallback avec structure minimale
        try:
            data_minimal = {
                "deal_link": deal_link,
                "titre": titre[:200],
                "temperature": temperature
            }
            
            result = supabase.table('deals_envoyes').upsert(data_minimal, on_conflict="deal_link").execute()
            print("üíæ Deal sauvegard√© avec structure minimale")
            return True
            
        except Exception as e2:
            print(f"‚ùå Erreur m√™me avec structure minimale : {e2}")
            return False

def debug_page_structure(url=None):
    """
    Fonction de debug pour analyser la structure de la page
    """
    url = url or DEALABS_URL
    print(f"üîç Mode debug activ√© pour : {url}")
    
    try:
        session = requests.Session()
        session.headers.update(HEADERS)
        response = session.get(url, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        if "/bons-plans/" in url:
            # Debug d'une page de deal sp√©cifique
            print("üîç Analyse d'une page de deal...")
            
            boutons_deal = soup.find_all('a', href=True)
            boutons_pertinents = []
            
            for bouton in boutons_deal:
                href = bouton.get('href', '')
                classes = ' '.join(bouton.get('class', []))
                text = bouton.get_text(strip=True)
                
                if any(keyword in href.lower() for keyword in ['out.php', '/visit/', 'amazon.fr', 'fnac.com']):
                    boutons_pertinents.append({
                        'href': href,
                        'classes': classes,
                        'text': text[:50]
                    })
            
            print(f"üéØ {len(boutons_pertinents)} boutons pertinents trouv√©s:")
            for i, bouton in enumerate(boutons_pertinents[:5]):
                print(f"  {i+1}. Classes: {bouton['classes']}")
                print(f"      Href: {bouton['href']}")
                print(f"      Text: {bouton['text']}")
                print()
        else:
            # Debug de la page principale
            all_links = soup.find_all('a', href=True)
            deal_links = [link for link in all_links if any(keyword in link.get('href', '').lower() for keyword in ['/deals/', '/bons-plans/', '/codes-promo/'])]
            
            print(f"üîó {len(all_links)} liens total, {len(deal_links)} liens de deals")
            
            if deal_links[:3]:
                print("üìã Premiers deals trouv√©s:")
                for i, link in enumerate(deal_links[:3]):
                    title = link.get_text(strip=True)[:60]
                    href = link['href']
                    print(f"  {i+1}. {title} -> {href}")
        
    except Exception as e:
        print(f"‚ùå Erreur debug : {e}")

# --- PROGRAMME PRINCIPAL ---
if __name__ == "__main__":
    print("ü§ñ D√©marrage du DealBot Pro avec mon√©tisation am√©lior√©e...")
    
    # V√©rifier les configurations
    if not DISCORD_WEBHOOK_URL or len(DISCORD_WEBHOOK_URL) < 50:
        print("‚ùå ERREUR: DISCORD_WEBHOOK_URL mal configur√©")
        exit(1)
        
    if not SUPABASE_URL or not SUPABASE_KEY:
        print("‚ùå ERREUR: Configuration Supabase manquante")
        exit(1)
    
    # Connexion √† Supabase
    try:
        supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
        print("‚úÖ Connect√© √† Supabase")
    except Exception as e:
        print(f"‚ùå Erreur connexion Supabase : {e}")
        exit(1)
    
    print(f"üå°Ô∏è Seuil de temp√©rature : {TEMPERATURE_MINIMUM}¬∞")
    print(f"üí∞ Tag Amazon : {AMAZON_TAG}")
    
    # D√©commentez pour debug
    # debug_page_structure()
    
    # --- WORKFLOW PRINCIPAL AVEC AFFILIATION AM√âLIOR√âE ---
    # 1. Trouver un deal
    titre, lien_dealabs, temperature = scraper_dealabs()
    
    if not titre or not lien_dealabs:
        print("‚ùå Aucun deal trouv√©")
        exit(0)
    
    # 2. V√âRIFICATION CRITIQUE : Le deal est-il nouveau ?
    if deal_existe_en_bdd(supabase, lien_dealabs):
        print("üõë Deal d√©j√† trait√©, arr√™t du processus pour √©viter doublon")
        exit(0)
    
    # 3. SAUVEGARDE IMM√âDIATE pour r√©server le deal
    print("üîí R√©servation du deal en BDD...")
    if not sauvegarder_deal_traite(supabase, lien_dealabs, titre, temperature):
        print("‚ùå Impossible de r√©server le deal, arr√™t pour √©viter les conflits")
        exit(1)
    
    # 4. R√©cup√©rer le lien marchand (version am√©lior√©e)
    lien_marchand = recuperer_lien_marchand(lien_dealabs)
    
    # 5. Le transformer en lien affili√© si possible
    lien_final = transformer_en_lien_affilie(lien_marchand) if lien_marchand else lien_dealabs
    
    # 6. Mettre √† jour la BDD avec le lien marchand trouv√©
    if lien_marchand:
        sauvegarder_deal_traite(supabase, lien_dealabs, titre, temperature, lien_final)
    
    # 7. Envoyer la notification avec les deux liens si n√©cessaire
    temp_emoji = get_temperature_emoji(temperature)
    print(f"üì§ Envoi du deal {temp_emoji} ({temperature}¬∞)...")
    
    success = envoyer_notification_discord(titre, lien_final, temperature, lien_dealabs)
    
    # 8. R√©sultat final
    if success:
        if lien_marchand and lien_marchand != lien_dealabs:
            if "amazon.fr" in lien_final and AMAZON_TAG in lien_final:
                print("üéâ Mission accomplie ! Deal envoy√© avec lien affili√© Amazon üí∞")
            else:
                print("üéâ Mission accomplie ! Deal envoy√© avec lien marchand direct üõí")
        else:
            print("üéâ Mission accomplie ! Deal envoy√© (lien Dealabs) üìã")
    else:
        print("‚ö†Ô∏è Erreur lors de l'envoi Discord, mais deal d√©j√† sauvegard√©")
    
    # 9. Stats finales
    print(f"\nüìä R√âSUM√â:")
    print(f"   üå°Ô∏è  Temp√©rature: {temperature}¬∞")
    print(f"   üîó Lien Dealabs: {lien_dealabs[:50]}...")
    print(f"   üõí Lien marchand: {'Trouv√© ‚úÖ' if lien_marchand else 'Non trouv√© ‚ùå'}")
    print(f"   üí∞ Affiliation: {'Activ√©e ‚úÖ' if lien_final and AMAZON_TAG in lien_final else 'Non applicable'}")